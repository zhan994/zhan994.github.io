<!DOCTYPE HTML>
<html lang="en">


<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhihao Zhan</title>

  <meta name="author" content="Zhihao Zhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zhihao Zhan
                  </p>
                  <p>I'm the <strong>Algorithm Leader</strong> at <a href="https://www.topxgun.com/"> TopXGun
                      (Nanjing) Robotics Co., Ltd.</a> in China, where I lead a small team that mostly works on
                    <strong>SLAM, Perception, 3D Reconstruction and Coverage Path Planning</strong> for UAV Autopilots.
                  </p>
                  <p>
                    I hold a M.Sc in Multimedia Infomation Technology from <a href="https://www.cityu.edu.hk/">City
                      University of Hong Kong</a>, where I was advised by <a
                      href="https://scholars.cityu.edu.hk/en/persons/ho-man-chan(7eb8975f-575f-4672-9a7f-56078d98a503).html">Prof.
                      Ho Man CHAN</a> and a B.Eng in Electronic Information Engineering from <a
                      href="https://en.njtech.edu.cn/">Nanjing Tech University</a>.
                  </p>
                  <p>
                    Mail: zhihazhan2-c [at] my [dot] cityu [dot] edu [dot] hk
                  </p>
                  <p style="text-align:center">
                    <a href="https://github.com/zhan994/">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=GEcK0R4AAAAJ&hl">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/zhan994/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/robot.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/robot.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in <strong>SLAM, Spatial AI, and Robotics</strong>.
                  </p>
                  <p style="font-size: 12px;">*: co-first author</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:40px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
                  <a href="images/icsps2025_vdm_vsr.png"><img src="images/icsps2025_vdm_vsr.png" alt="VDM-VSR"
                      width="300"></a>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion
                    Alignment
                  </papertitle>
                  <br>
                  <strong>Zhihao Zhan*</strong>, Wang Pang*, Xiang Zhu*, and Yechao Bai
                  <br>
                  <em>17th International Conference on Signal Processing Systems (ICSPS)</em>, 2025,
                  <strong>Oral</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2503.03355">arXiv</a>
                  <br>
                  <p>
                    We rethink the approach to video super-resolution by introducing a method based on the Diffusion
                    Posterior Sampling framework, combined with an unconditional video diffusion transformer operating
                    in latent space. The video generation model, a diffusion transformer, functions as a space-time
                    model. We argue that a powerful model, which learns the physics of the real world, can easily handle
                    various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit
                    estimation of optical flows or motion parameters for pixel alignment.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding-left:40px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
                  <a href="images/iros2025_fusion_ortho.png"><img src="images/iros2025_fusion_ortho.png"
                      alt="Fusion-Ortho" width="300"></a>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping
                  </papertitle>
                  <br>
                  Jialei He*, <strong>Zhihao Zhan*</strong>, Zhituo Tu, Xiang Zhu, and Jie Yuan
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2025,
                  <strong>Oral</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2503.01202">arXiv</a>
                  <br>
                  <p>
                    In this paper, we utilize multi-sensor data to overcome the limitations of conventional orthoimage
                    generation methods in terms of temporal performance, system robustness, and geographic reference
                    accuracy. A prior-pose-optimized feature matching method is introduced to enhance matching speed and
                    accuracy, reducing the number of required features and providing precise references for the
                    Structure from Motion (SfM) process. The proposed method exhibits robustness in low-texture scenes
                    like farmlands, where feature matching is difficult.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding-left:40px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
                  <a href="images/icip2025_bair_examples.png"><img src="images/icip2025_bair_examples.png" alt="VDM-MD"
                      width="300"></a>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Image Motion Blur Removal in the Temporal Dimension with Video Diffusion Models
                  </papertitle>
                  <br>
                  Wang Pang*, <strong>Zhihao Zhan*</strong>, Xiang Zhu*, and Yechao Bai
                  <br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2025
                  <br>
                  <a href="https://zhan994.github.io/temporal-motion-deblur/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2501.12604">arXiv</a>
                  /
                  <a href="https://github.com/zhan994/temporal-motion-deblur">code</a>
                  <br>
                  <p>
                    We propose a novel single-image deblurring approach that treats motion blur as a temporal averaging
                    phenomenon. Our core innovation lies in leveraging a pre-trained video diffusion transformer model
                    to capture diverse motion dynamics within a latent space. It sidesteps explicit kernel estimation
                    and effectively accommodates diverse motion patterns.
                  </p>
                </td>
              </tr>

              <!-- <tr>
                <td style="padding-left:40px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
                  <a href="images/lidaruda.png"><img src="images/lidaruda.png" alt="LiDAR-UDA" width="200"></a>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation
                  </papertitle>
                  <br>
                  Amirreza Shaban*, JoonHo Lee*, <strong>Sanghun Jung*</strong>, Xiangyun Meng, and Byron Boots
                  <br>
                  <em>ICCV</em>, 2023 &nbsp <strong>Oral(1.8%) </strong>
                  <br>
                  <a href="https://arxiv.org/abs/2309.13523">arXiv</a>
                  /
                  <a href="https://github.com/JHLee0513/LiDARUDA">code</a>
                </td>
              </tr> -->

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Academic Services</h2>
                  <p>
                    <strong>Conference Reviewer</strong>
                  </p>
                  <ul>
                    <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS): 2025</li>
                    <li>IEEE International Conference on Robotics and Automation (ICRA): 2026</li>
                  </ul>
                  <p>
                    <strong>Journal Reviewer</strong>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    This website is adapted from Jon Barron's template.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
  </table>
</body>


</html>